{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detailed Pipeline Walkthrough\n",
    "\n",
    "**A step-by-step guide to running the Fly Behavior Pipeline from a Jupyter notebook.**\n",
    "\n",
    "This notebook mirrors exactly what `make run` does, but breaks it into clearly separated,\n",
    "well-explained stages so you can understand, inspect, and control each part.\n",
    "\n",
    "---\n",
    "\n",
    "### What the pipeline does (high level)\n",
    "\n",
    "1. **YOLO inference** — detect eyes and proboscis in each video frame\n",
    "2. **Distance / angle computation** — derive per-frame CSV metrics\n",
    "3. **Normalization** — convert raw distances to percentages\n",
    "4. **RMS / envelope analysis** — compute rolling-window smoothed signals\n",
    "5. **Combined analysis** — merge datasets, build matrices, generate plots\n",
    "6. **Reaction prediction** — run trained ML model on combined data\n",
    "7. **Backup** — copy results to SMB / Box / secured storage\n",
    "\n",
    "### Safety defaults in this notebook\n",
    "\n",
    "| Setting | Default | Why |\n",
    "|---------|---------|-----|\n",
    "| `DRY_RUN_ONLY` | `True` | Validates everything without running the actual pipeline |\n",
    "| `RUN_BACKUPS` | `False` | Backups are skipped unless you explicitly enable them |\n",
    "| `delete_source_after_render` | **forced to `false`** | We never delete source videos from inside this notebook |\n",
    "\n",
    "> **Tip:** Run this notebook top-to-bottom once with `DRY_RUN_ONLY = True` to verify\n",
    "> your environment, then set it to `False` for the real run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## A) Parameters\n",
    "\n",
    "Edit the values in this cell to control the notebook's behavior.\n",
    "Everything else reads from these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PARAMETERS — edit these before running the notebook\n",
    "# ===================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to your config YAML (relative to repo root, or absolute)\n",
    "CONFIG_PATH = Path(\"config/config.yaml\")\n",
    "\n",
    "# Set to False to actually execute the pipeline (make run).\n",
    "# When True, the notebook only validates the environment and config.\n",
    "DRY_RUN_ONLY = True\n",
    "\n",
    "# Set to True to run backups after the pipeline finishes.\n",
    "# Backups copy results to SMB, Box, and secured storage.\n",
    "RUN_BACKUPS = False\n",
    "\n",
    "# Allow the pipeline to run on CPU (no GPU).\n",
    "# If False and no GPU is found, the pipeline will refuse to start.\n",
    "ALLOW_CPU = True\n",
    "\n",
    "# Directory where pipeline log files are saved.\n",
    "LOG_DIR = Path(\"logs\")\n",
    "\n",
    "# Maximum seconds to wait for the pipeline before timing out.\n",
    "# Default: 6 hours (21600 seconds). Set to None for no limit.\n",
    "PIPELINE_TIMEOUT_SECONDS = 21600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## B) Environment Validation\n",
    "\n",
    "This section checks that all required software and packages are available\n",
    "before you try to run anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Environment validation — checks Python, packages, and tools\n",
    "# ===================================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "# ---------- Resolve repo root ----------\n",
    "# This notebook lives in docs/, so the repo root is one level up.\n",
    "_notebook_dir = Path(os.getcwd())\n",
    "# If running from the repo root already, use cwd; otherwise go up.\n",
    "if (_notebook_dir / \"Makefile\").exists():\n",
    "    REPO_ROOT = _notebook_dir\n",
    "elif (_notebook_dir.parent / \"Makefile\").exists():\n",
    "    REPO_ROOT = _notebook_dir.parent\n",
    "else:\n",
    "    # Fallback: use the notebook's grandparent\n",
    "    REPO_ROOT = _notebook_dir.parents[1] if len(_notebook_dir.parents) > 1 else _notebook_dir\n",
    "\n",
    "print(f\"Repo root : {REPO_ROOT}\")\n",
    "print(f\"Python    : {sys.version}\")\n",
    "print()\n",
    "\n",
    "# ---------- Check required Python packages ----------\n",
    "required_packages = [\n",
    "    (\"yaml\",       \"PyYAML\"),\n",
    "    (\"numpy\",      \"numpy\"),\n",
    "    (\"pandas\",     \"pandas\"),\n",
    "    (\"matplotlib\", \"matplotlib\"),\n",
    "    (\"cv2\",        \"opencv-python\"),\n",
    "    (\"ultralytics\",\"ultralytics\"),\n",
    "    (\"torch\",      \"torch (PyTorch)\"),\n",
    "    (\"scipy\",      \"scipy\"),\n",
    "    (\"tqdm\",       \"tqdm\"),\n",
    "]\n",
    "\n",
    "all_ok = True\n",
    "for module_name, display_name in required_packages:\n",
    "    try:\n",
    "        __import__(module_name)\n",
    "        print(f\"  [OK]  {display_name}\")\n",
    "    except ImportError:\n",
    "        print(f\"  [MISSING]  {display_name}  <-- install with: pip install {display_name}\")\n",
    "        all_ok = False\n",
    "\n",
    "print()\n",
    "\n",
    "# ---------- Check external CLI tools ----------\n",
    "cli_tools = [\n",
    "    (\"ffmpeg\",  \"Video encoding (used by compose_videos_rms)\"),\n",
    "    (\"ffprobe\", \"Video metadata inspection\"),\n",
    "    (\"make\",    \"Build system (runs the pipeline)\"),\n",
    "]\n",
    "\n",
    "for tool_name, purpose in cli_tools:\n",
    "    path = shutil.which(tool_name)\n",
    "    if path:\n",
    "        print(f\"  [OK]  {tool_name} -> {path}\")\n",
    "    else:\n",
    "        print(f\"  [MISSING]  {tool_name} ({purpose})\")\n",
    "        all_ok = False\n",
    "\n",
    "print()\n",
    "\n",
    "# ---------- GPU check ----------\n",
    "try:\n",
    "    import torch\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    if gpu_available:\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        print(f\"  [GPU]  CUDA available: {gpu_name}\")\n",
    "    else:\n",
    "        print(f\"  [CPU]  No CUDA GPU detected.\")\n",
    "        if not ALLOW_CPU:\n",
    "            print(\"         ERROR: ALLOW_CPU is False — set it to True or use a GPU machine.\")\n",
    "            all_ok = False\n",
    "        else:\n",
    "            print(\"         Pipeline will run on CPU (slower but functional).\")\n",
    "except ImportError:\n",
    "    print(\"  [CPU]  PyTorch not installed — GPU detection skipped.\")\n",
    "\n",
    "print()\n",
    "if all_ok:\n",
    "    print(\"Environment validation PASSED.\")\n",
    "else:\n",
    "    print(\"Environment validation FAILED — fix the issues above before proceeding.\")\n",
    "    raise SystemExit(\"Environment check failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## C) Config Selection and Validation\n",
    "\n",
    "This section loads your `config.yaml`, prints the key settings, and warns\n",
    "you about any destructive options that are currently enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Load and validate config.yaml\n",
    "# ===================================================================\n",
    "\n",
    "import yaml\n",
    "\n",
    "# Resolve config path relative to repo root\n",
    "config_file = REPO_ROOT / CONFIG_PATH\n",
    "if not config_file.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Config file not found: {config_file}\\n\"\n",
    "        f\"Check your CONFIG_PATH parameter. Available configs:\\n\"\n",
    "        + \"\\n\".join(f\"  - {p.name}\" for p in (REPO_ROOT / \"config\").glob(\"*.yaml\"))\n",
    "    )\n",
    "\n",
    "with open(config_file, \"r\") as fh:\n",
    "    config_data = yaml.safe_load(fh) or {}\n",
    "\n",
    "print(f\"Loaded config: {config_file}\")\n",
    "print(f\"Config size  : {config_file.stat().st_size:,} bytes\")\n",
    "print()\n",
    "\n",
    "# ---------- Print key settings ----------\n",
    "print(\"=\" * 60)\n",
    "print(\"EFFECTIVE CONFIGURATION (key values)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Model path\n",
    "model_path = config_data.get(\"model_path\", \"(not set)\")\n",
    "model_exists = Path(str(model_path)).expanduser().exists() if model_path else False\n",
    "print(f\"  model_path             : {model_path}\")\n",
    "print(f\"    exists?              : {model_exists}\")\n",
    "\n",
    "# Dataset roots\n",
    "main_dirs = config_data.get(\"main_directories\", [])\n",
    "if isinstance(main_dirs, str):\n",
    "    main_dirs = [main_dirs]\n",
    "print(f\"  main_directories       : {len(main_dirs)} dataset(s)\")\n",
    "for d in main_dirs:\n",
    "    exists = Path(d).expanduser().exists()\n",
    "    status = \"OK\" if exists else \"MISSING\"\n",
    "    print(f\"    [{status}] {d}\")\n",
    "\n",
    "# Timing\n",
    "print(f\"  fps_default            : {config_data.get('fps_default', '(not set)')}\")\n",
    "print(f\"  odor_on_s              : {config_data.get('odor_on_s', '(not set)')}\")\n",
    "print(f\"  odor_off_s             : {config_data.get('odor_off_s', '(not set)')}\")\n",
    "print(f\"  odor_latency_s         : {config_data.get('odor_latency_s', '(not set)')}\")\n",
    "\n",
    "# GPU / compute\n",
    "print(f\"  allow_cpu              : {config_data.get('allow_cpu', False)}\")\n",
    "print(f\"  cuda_allow_tf32        : {config_data.get('cuda_allow_tf32', True)}\")\n",
    "\n",
    "# Force flags\n",
    "force = config_data.get(\"force\", {})\n",
    "print(f\"  force.pipeline         : {force.get('pipeline', False)}\")\n",
    "print(f\"  force.yolo             : {force.get('yolo', False)}\")\n",
    "print(f\"  force.combined         : {force.get('combined', False)}\")\n",
    "print(f\"  force.reaction_predict : {force.get('reaction_prediction', False)}\")\n",
    "print(f\"  force.reaction_matrix  : {force.get('reaction_matrix', False)}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ---------- SAFETY WARNINGS ----------\n",
    "print(\"=\" * 60)\n",
    "print(\"SAFETY CHECK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "warnings_found = 0\n",
    "\n",
    "# Check delete_source_after_render\n",
    "delete_source = config_data.get(\"delete_source_after_render\", False)\n",
    "if delete_source:\n",
    "    warnings_found += 1\n",
    "    print(f\"  [WARNING] delete_source_after_render = {delete_source}\")\n",
    "    print(f\"            This DELETES original video files after rendering!\")\n",
    "    print(f\"            The notebook will override this to 'false' at runtime.\")\n",
    "else:\n",
    "    print(f\"  [SAFE]    delete_source_after_render = {delete_source}\")\n",
    "\n",
    "# Check secure_cleanup (moves files to secured storage)\n",
    "secure_cfg = (config_data.get(\"analysis\", {}).get(\"combined\", {}) or {}).get(\"secure_cleanup\")\n",
    "if secure_cfg:\n",
    "    perform_cleanup = secure_cfg.get(\"perform_cleanup\", True)\n",
    "    if perform_cleanup:\n",
    "        warnings_found += 1\n",
    "        print(f\"  [WARNING] secure_cleanup.perform_cleanup = True\")\n",
    "        print(f\"            This moves/deletes files after copying to secured storage.\")\n",
    "    else:\n",
    "        print(f\"  [SAFE]    secure_cleanup.perform_cleanup = False\")\n",
    "\n",
    "# Check backup compression\n",
    "backup_cfg = config_data.get(\"backups\", {})\n",
    "compression = backup_cfg.get(\"compression\", {}).get(\"enabled\", False)\n",
    "print(f\"  [INFO]    backups.compression.enabled = {compression}\")\n",
    "\n",
    "if warnings_found > 0:\n",
    "    print(f\"\\n  >>> {warnings_found} warning(s) found. Review above before running. <<<\")\n",
    "else:\n",
    "    print(f\"\\n  All safety checks passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## D) Non-Destructive Overrides\n",
    "\n",
    "This cell sets environment variables that make the pipeline safer.\n",
    "These overrides apply **only** for this notebook session — your config.yaml is not modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Apply safe runtime overrides (environment variables only)\n",
    "# ===================================================================\n",
    "\n",
    "# Force matplotlib to use the non-interactive Agg backend.\n",
    "# This prevents the pipeline from trying to open GUI windows.\n",
    "os.environ[\"MPLBACKEND\"] = \"Agg\"\n",
    "print(\"Set MPLBACKEND=Agg (headless plotting)\")\n",
    "\n",
    "# Suppress noisy ONNX Runtime logging.\n",
    "os.environ[\"ORT_LOGGING_LEVEL\"] = \"3\"\n",
    "print(\"Set ORT_LOGGING_LEVEL=3 (warnings only)\")\n",
    "\n",
    "# Override CPU usage if the parameter allows it.\n",
    "if ALLOW_CPU:\n",
    "    os.environ[\"ALLOW_CPU\"] = \"1\"\n",
    "    print(\"Set ALLOW_CPU=1 (pipeline can fall back to CPU)\")\n",
    "\n",
    "# Ensure logs directory exists.\n",
    "log_dir = REPO_ROOT / LOG_DIR\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Log directory ready: {log_dir}\")\n",
    "\n",
    "print()\n",
    "print(\"Runtime overrides applied. Your config.yaml is unchanged.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## E) Dry-Run Checks\n",
    "\n",
    "Before running the pipeline, let's verify:\n",
    "- Input dataset directories exist and contain data\n",
    "- The YOLO model file exists\n",
    "- What outputs will be generated (and where)\n",
    "\n",
    "This runs even when `DRY_RUN_ONLY = True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Dry-run checks — verify inputs and show expected outputs\n",
    "# ===================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DRY-RUN CHECKS\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# ---------- 1) Check dataset directories ----------\n",
    "print(\"--- Dataset Directories ---\")\n",
    "datasets_ready = 0\n",
    "datasets_missing = 0\n",
    "\n",
    "for d in main_dirs:\n",
    "    p = Path(d).expanduser()\n",
    "    if p.exists():\n",
    "        # Count fly directories (immediate subdirectories)\n",
    "        fly_dirs = [x for x in p.iterdir() if x.is_dir()]\n",
    "        # Count CSV files recursively\n",
    "        csv_count = sum(1 for _ in p.rglob(\"*.csv\"))\n",
    "        print(f\"  [OK]  {p.name}: {len(fly_dirs)} fly dir(s), {csv_count} CSV(s)\")\n",
    "        datasets_ready += 1\n",
    "    else:\n",
    "        print(f\"  [MISSING] {d}\")\n",
    "        datasets_missing += 1\n",
    "\n",
    "print(f\"  Summary: {datasets_ready} ready, {datasets_missing} missing\")\n",
    "print()\n",
    "\n",
    "# ---------- 2) Check YOLO model ----------\n",
    "print(\"--- YOLO Model ---\")\n",
    "model_file = Path(str(model_path)).expanduser()\n",
    "if model_file.exists():\n",
    "    size_mb = model_file.stat().st_size / (1024 * 1024)\n",
    "    mtime = datetime.fromtimestamp(model_file.stat().st_mtime)\n",
    "    print(f\"  [OK]  {model_file.name} ({size_mb:.1f} MB, modified {mtime:%Y-%m-%d %H:%M})\")\n",
    "else:\n",
    "    print(f\"  [MISSING] {model_file}\")\n",
    "    print(f\"            Pipeline will fail at YOLO step. Check model_path in config.\")\n",
    "print()\n",
    "\n",
    "# ---------- 3) Show expected output locations ----------\n",
    "print(\"--- Expected Output Locations ---\")\n",
    "analysis_cfg = config_data.get(\"analysis\", {})\n",
    "\n",
    "output_locations = []\n",
    "\n",
    "# Envelope visuals\n",
    "env_vis = analysis_cfg.get(\"envelope_visuals\", {})\n",
    "if env_vis:\n",
    "    for key in (\"matrices\", \"envelopes\"):\n",
    "        sub = env_vis.get(key)\n",
    "        if sub:\n",
    "            items = sub if isinstance(sub, list) else [sub]\n",
    "            for item in items:\n",
    "                if \"out_dir\" in item:\n",
    "                    output_locations.append((\"envelope_visuals.\" + key, item[\"out_dir\"]))\n",
    "\n",
    "# Combined outputs\n",
    "combined = analysis_cfg.get(\"combined\", {})\n",
    "if combined:\n",
    "    for key in (\"matrices\", \"matrix\", \"overlay\"):\n",
    "        sub = combined.get(key)\n",
    "        if sub and \"out_dir\" in sub:\n",
    "            output_locations.append((\"combined.\" + key, sub[\"out_dir\"]))\n",
    "    envelopes = combined.get(\"envelopes\", [])\n",
    "    if not isinstance(envelopes, list):\n",
    "        envelopes = [envelopes]\n",
    "    for env in envelopes:\n",
    "        if isinstance(env, dict) and \"out_dir\" in env:\n",
    "            output_locations.append((\"combined.envelopes\", env[\"out_dir\"]))\n",
    "    wide = combined.get(\"wide\", {})\n",
    "    if wide and \"output_csv\" in wide:\n",
    "        output_locations.append((\"combined.wide\", wide[\"output_csv\"]))\n",
    "\n",
    "# Reaction predictions\n",
    "reaction = config_data.get(\"reaction_prediction\", {})\n",
    "if reaction:\n",
    "    if \"output_csv\" in reaction:\n",
    "        output_locations.append((\"reaction_prediction\", reaction[\"output_csv\"]))\n",
    "    matrix = reaction.get(\"matrix\", {})\n",
    "    if matrix and \"out_dir\" in matrix:\n",
    "        output_locations.append((\"reaction_prediction.matrix\", matrix[\"out_dir\"]))\n",
    "\n",
    "for label, location in output_locations:\n",
    "    loc_path = Path(str(location)).expanduser()\n",
    "    exists = loc_path.exists()\n",
    "    status = \"exists\" if exists else \"will be created\"\n",
    "    print(f\"  {label}\")\n",
    "    print(f\"    -> {location} ({status})\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ---------- 4) Show the exact command that will run ----------\n",
    "print(\"--- Pipeline Command ---\")\n",
    "pipeline_cmd = f\"make run\"  # Uses Makefile target\n",
    "print(f\"  Command  : {pipeline_cmd}\")\n",
    "print(f\"  Expands to:\")\n",
    "print(f\"    export MPLBACKEND=Agg\")\n",
    "print(f\"    export ORT_LOGGING_LEVEL=3\")\n",
    "print(f\"    python scripts/pipeline/run_workflows.py --config {CONFIG_PATH}\")\n",
    "if RUN_BACKUPS:\n",
    "    print(f\"    python scripts/backup_system.py\")\n",
    "else:\n",
    "    print(f\"    (backups skipped — RUN_BACKUPS = False)\")\n",
    "print()\n",
    "\n",
    "if DRY_RUN_ONLY:\n",
    "    print(\"DRY_RUN_ONLY = True -> The pipeline will NOT execute.\")\n",
    "    print(\"Set DRY_RUN_ONLY = False in the Parameters cell to run for real.\")\n",
    "else:\n",
    "    print(\"DRY_RUN_ONLY = False -> The pipeline WILL execute in the next section.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## F) Run the Pipeline\n",
    "\n",
    "This is the main execution cell. It runs `make run` (minus the backup step\n",
    "if `RUN_BACKUPS` is `False`).\n",
    "\n",
    "**What to expect:**\n",
    "- Output streams in real-time below the cell\n",
    "- A timestamped log file is saved to the `logs/` directory\n",
    "- If anything fails, the cell stops and shows the last 50 lines of output\n",
    "- The pipeline can take 15–60+ minutes depending on your data size and hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Shell runner helper — streams output and logs to file\n",
    "# ===================================================================\n",
    "\n",
    "def run_shell_command(\n",
    "    cmd,\n",
    "    *,\n",
    "    cwd=None,\n",
    "    env=None,\n",
    "    log_file=None,\n",
    "    timeout=None,\n",
    "    label=\"command\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Run a shell command with real-time output streaming.\n",
    "\n",
    "    - Streams stdout/stderr to the notebook AND to a log file.\n",
    "    - On failure: prints the last 50 lines and raises an exception.\n",
    "    - Returns the process return code (0 = success).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cmd : list of str\n",
    "        The command to run (e.g., [\"make\", \"run\"]).\n",
    "    cwd : Path or str, optional\n",
    "        Working directory. Defaults to REPO_ROOT.\n",
    "    env : dict, optional\n",
    "        Environment variables. Defaults to current env.\n",
    "    log_file : Path or str, optional\n",
    "        Path to write all output. Created automatically if not given.\n",
    "    timeout : int, optional\n",
    "        Max seconds to wait. None = no limit.\n",
    "    label : str\n",
    "        Human-readable name for the command (used in messages).\n",
    "    \"\"\"\n",
    "    if cwd is None:\n",
    "        cwd = REPO_ROOT\n",
    "    if env is None:\n",
    "        env = os.environ.copy()\n",
    "    if log_file is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        log_file = log_dir / f\"pipeline_run_{timestamp}.log\"\n",
    "\n",
    "    log_file = Path(log_file)\n",
    "    log_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"Running: {' '.join(str(c) for c in cmd)}\")\n",
    "    print(f\"Log    : {log_file}\")\n",
    "    print(f\"Started: {datetime.now():%Y-%m-%d %H:%M:%S}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    collected_lines = []\n",
    "\n",
    "    try:\n",
    "        process = subprocess.Popen(\n",
    "            cmd,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            text=True,\n",
    "            bufsize=1,\n",
    "            cwd=str(cwd),\n",
    "            env=env,\n",
    "        )\n",
    "\n",
    "        with open(log_file, \"w\") as lf:\n",
    "            for line in process.stdout:\n",
    "                print(line, end=\"\", flush=True)\n",
    "                lf.write(line)\n",
    "                lf.flush()\n",
    "                collected_lines.append(line)\n",
    "\n",
    "        returncode = process.wait(timeout=timeout)\n",
    "\n",
    "    except subprocess.TimeoutExpired:\n",
    "        process.kill()\n",
    "        print(f\"\\nTIMEOUT: {label} exceeded {timeout}s limit.\")\n",
    "        returncode = -1\n",
    "    except Exception as exc:\n",
    "        print(f\"\\nERROR running {label}: {exc}\")\n",
    "        returncode = -1\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Finished: {datetime.now():%Y-%m-%d %H:%M:%S}\")\n",
    "    print(f\"Exit code: {returncode}\")\n",
    "\n",
    "    if returncode != 0:\n",
    "        # Show last 50 lines for debugging\n",
    "        print()\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"FAILED — last 50 lines of {label} output:\")\n",
    "        print(\"=\" * 60)\n",
    "        for line in collected_lines[-50:]:\n",
    "            print(line, end=\"\")\n",
    "        print()\n",
    "        raise RuntimeError(\n",
    "            f\"{label} failed with exit code {returncode}. \"\n",
    "            f\"Full log: {log_file}\"\n",
    "        )\n",
    "\n",
    "    return returncode\n",
    "\n",
    "\n",
    "print(\"Shell runner helper defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Execute the pipeline (or skip if DRY_RUN_ONLY)\n",
    "# ===================================================================\n",
    "\n",
    "if DRY_RUN_ONLY:\n",
    "    print(\"DRY_RUN_ONLY = True — skipping pipeline execution.\")\n",
    "    print(\"Change DRY_RUN_ONLY to False in the Parameters cell and re-run.\")\n",
    "else:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"EXECUTING PIPELINE\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "\n",
    "    # Build the command.\n",
    "    # Instead of 'make run' (which includes backup), we call the pipeline\n",
    "    # directly so we can control backups separately.\n",
    "    pipeline_command = [\n",
    "        sys.executable,\n",
    "        \"scripts/pipeline/run_workflows.py\",\n",
    "        \"--config\",\n",
    "        str(CONFIG_PATH),\n",
    "    ]\n",
    "\n",
    "    # Build env with the same vars that Makefile sets.\n",
    "    run_env = os.environ.copy()\n",
    "    run_env[\"MPLBACKEND\"] = \"Agg\"\n",
    "    run_env[\"ORT_LOGGING_LEVEL\"] = \"3\"\n",
    "\n",
    "    # Add src/ to PYTHONPATH so imports resolve correctly.\n",
    "    src_path = str(REPO_ROOT / \"src\")\n",
    "    existing_pythonpath = run_env.get(\"PYTHONPATH\", \"\")\n",
    "    run_env[\"PYTHONPATH\"] = os.pathsep.join(\n",
    "        filter(None, [src_path, str(REPO_ROOT), existing_pythonpath])\n",
    "    )\n",
    "\n",
    "    run_shell_command(\n",
    "        pipeline_command,\n",
    "        env=run_env,\n",
    "        timeout=PIPELINE_TIMEOUT_SECONDS,\n",
    "        label=\"pipeline\",\n",
    "    )\n",
    "\n",
    "    print()\n",
    "    print(\"Pipeline completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## G) Output Summary\n",
    "\n",
    "After the pipeline finishes, this cell shows you:\n",
    "- Where outputs were saved\n",
    "- How many files were generated\n",
    "- Quick sanity checks on the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Output summary — inspect what was generated\n",
    "# ===================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"OUTPUT SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "for label, location in output_locations:\n",
    "    loc_path = Path(str(location)).expanduser()\n",
    "    print(f\"  {label}:\")\n",
    "    print(f\"    Path: {loc_path}\")\n",
    "\n",
    "    if not loc_path.exists():\n",
    "        print(f\"    Status: does not exist (not yet generated)\")\n",
    "    elif loc_path.is_file():\n",
    "        size_mb = loc_path.stat().st_size / (1024 * 1024)\n",
    "        mtime = datetime.fromtimestamp(loc_path.stat().st_mtime)\n",
    "        print(f\"    Status: file, {size_mb:.2f} MB, modified {mtime:%Y-%m-%d %H:%M}\")\n",
    "    elif loc_path.is_dir():\n",
    "        all_files = list(loc_path.rglob(\"*\"))\n",
    "        file_count = sum(1 for f in all_files if f.is_file())\n",
    "        total_bytes = sum(f.stat().st_size for f in all_files if f.is_file())\n",
    "        total_mb = total_bytes / (1024 * 1024)\n",
    "        print(f\"    Status: directory, {file_count} file(s), {total_mb:.1f} MB total\")\n",
    "        # Show the 5 most recently modified files\n",
    "        recent = sorted(\n",
    "            [f for f in all_files if f.is_file()],\n",
    "            key=lambda f: f.stat().st_mtime,\n",
    "            reverse=True,\n",
    "        )[:5]\n",
    "        if recent:\n",
    "            print(f\"    Recent files:\")\n",
    "            for f in recent:\n",
    "                mtime = datetime.fromtimestamp(f.stat().st_mtime)\n",
    "                print(f\"      {f.name} ({mtime:%Y-%m-%d %H:%M})\")\n",
    "    print()\n",
    "\n",
    "# ---------- Quick sanity check on CSVs ----------\n",
    "print(\"--- CSV Sanity Check ---\")\n",
    "csv_outputs = [loc for label, loc in output_locations if str(loc).endswith(\".csv\")]\n",
    "for csv_path_str in csv_outputs:\n",
    "    csv_path = Path(str(csv_path_str)).expanduser()\n",
    "    if csv_path.exists():\n",
    "        try:\n",
    "            import pandas as pd\n",
    "            df = pd.read_csv(csv_path, nrows=5)\n",
    "            print(f\"  {csv_path.name}: {len(df.columns)} columns, preview OK\")\n",
    "            print(f\"    Columns: {', '.join(df.columns[:8])}{'...' if len(df.columns) > 8 else ''}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  {csv_path.name}: ERROR reading — {e}\")\n",
    "    else:\n",
    "        print(f\"  {csv_path.name}: not found (pipeline may not have run yet)\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ---------- Log file info ----------\n",
    "print(\"--- Log Files ---\")\n",
    "log_files = sorted(log_dir.glob(\"pipeline_run_*.log\"), reverse=True)\n",
    "if log_files:\n",
    "    latest = log_files[0]\n",
    "    size_kb = latest.stat().st_size / 1024\n",
    "    print(f\"  Latest: {latest.name} ({size_kb:.1f} KB)\")\n",
    "    print(f\"  Total log files: {len(log_files)}\")\n",
    "else:\n",
    "    print(f\"  No pipeline log files found in {log_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## H) Backups (Optional)\n",
    "\n",
    "Backups are **off by default** (`RUN_BACKUPS = False`). When enabled, the\n",
    "backup system:\n",
    "\n",
    "1. Copies CSV results to the SMB network share\n",
    "2. Syncs to Box cloud storage via rclone\n",
    "3. Copies to secured local storage\n",
    "\n",
    "**No compression by default** — files are copied directly. Enable compression\n",
    "in `config.yaml` under `backups.compression.enabled`.\n",
    "\n",
    "To run backups: set `RUN_BACKUPS = True` in the Parameters cell, then re-run\n",
    "this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Run backups (only if RUN_BACKUPS is True)\n",
    "# ===================================================================\n",
    "\n",
    "if not RUN_BACKUPS:\n",
    "    print(\"RUN_BACKUPS = False — skipping backups.\")\n",
    "    print(\"Set RUN_BACKUPS = True in the Parameters cell to enable.\")\n",
    "    print()\n",
    "    print(\"Backup destinations configured in config.yaml:\")\n",
    "    destinations = backup_cfg.get(\"destinations\", {})\n",
    "    for name, dest in destinations.items():\n",
    "        enabled = dest.get(\"enabled\", False)\n",
    "        status = \"enabled\" if enabled else \"disabled\"\n",
    "        print(f\"  {name}: {status}\")\n",
    "else:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"RUNNING BACKUPS\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "\n",
    "    backup_command = [sys.executable, \"scripts/backup_system.py\"]\n",
    "\n",
    "    run_shell_command(\n",
    "        backup_command,\n",
    "        timeout=3600,  # 1 hour max for backups\n",
    "        label=\"backup\",\n",
    "    )\n",
    "\n",
    "    print()\n",
    "    print(\"Backups completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## I) Troubleshooting\n",
    "\n",
    "### Common errors and what to do\n",
    "\n",
    "| Error | Cause | Fix |\n",
    "|-------|-------|-----|\n",
    "| `FileNotFoundError: config.yaml` | Config path is wrong | Update `CONFIG_PATH` in Parameters cell |\n",
    "| `ModuleNotFoundError: ultralytics` | Missing package | Run `pip install -r requirements.txt` |\n",
    "| `CUDA out of memory` | GPU memory full | Close other GPU processes, or set `allow_cpu: true` in config |\n",
    "| `No such file: best.pt` | Model file missing | Check `model_path` in config.yaml |\n",
    "| `RuntimeError: pipeline failed` | General pipeline error | Check the log file path shown in the error message |\n",
    "| `OSError: [Errno 28] No space` | Disk full | Free up disk space, check `df -h` |\n",
    "| `Permission denied` on SMB/secured | Network mount not available | Check that SMB share is mounted |\n",
    "\n",
    "### View the latest log file\n",
    "\n",
    "Run the cell below to see the last 50 lines of the most recent pipeline log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# View latest log file (last 50 lines)\n",
    "# ===================================================================\n",
    "\n",
    "log_files = sorted(log_dir.glob(\"pipeline_run_*.log\"), reverse=True)\n",
    "\n",
    "if log_files:\n",
    "    latest_log = log_files[0]\n",
    "    size_kb = latest_log.stat().st_size / 1024\n",
    "    mtime = datetime.fromtimestamp(latest_log.stat().st_mtime)\n",
    "\n",
    "    print(f\"Latest log: {latest_log.name}\")\n",
    "    print(f\"Size: {size_kb:.1f} KB | Modified: {mtime:%Y-%m-%d %H:%M:%S}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    with open(latest_log, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    print(f\"Showing last 50 of {len(lines)} lines:\\n\")\n",
    "    for line in lines[-50:]:\n",
    "        print(line, end=\"\")\n",
    "else:\n",
    "    print(\"No pipeline log files found.\")\n",
    "    print(f\"Logs are saved to: {log_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Run All (Sequential)\n",
    "\n",
    "This cell runs every stage above in order. Useful for a completely\n",
    "hands-off execution once you've verified the dry-run checks.\n",
    "\n",
    "**Important:** Make sure you've set `DRY_RUN_ONLY = False` in the Parameters\n",
    "cell before running this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Run All — sequential execution of the full workflow\n",
    "# ===================================================================\n",
    "# This cell is a convenience wrapper. Each stage is identical to\n",
    "# running the cells above in order.\n",
    "#\n",
    "# Stages:\n",
    "#   1. Validate environment\n",
    "#   2. Load and check config\n",
    "#   3. Apply safe overrides\n",
    "#   4. Dry-run checks\n",
    "#   5. Run pipeline (if DRY_RUN_ONLY = False)\n",
    "#   6. Run backups (if RUN_BACKUPS = True)\n",
    "#   7. Print output summary\n",
    "#\n",
    "# If any stage fails, execution stops immediately.\n",
    "# ===================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"RUN ALL — started at {datetime.now():%Y-%m-%d %H:%M:%S}\")\n",
    "print(f\"DRY_RUN_ONLY = {DRY_RUN_ONLY}\")\n",
    "print(f\"RUN_BACKUPS  = {RUN_BACKUPS}\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "if DRY_RUN_ONLY:\n",
    "    print(\"DRY_RUN_ONLY is True — will validate but not execute.\")\n",
    "    print(\"Set DRY_RUN_ONLY = False in the Parameters cell for a real run.\")\n",
    "    print()\n",
    "    print(\"Validation completed above. Review the output in the cells above.\")\n",
    "else:\n",
    "    # --- Run pipeline ---\n",
    "    print(\"[Stage 1/3] Running pipeline...\")\n",
    "    pipeline_command = [\n",
    "        sys.executable,\n",
    "        \"scripts/pipeline/run_workflows.py\",\n",
    "        \"--config\",\n",
    "        str(CONFIG_PATH),\n",
    "    ]\n",
    "    run_env = os.environ.copy()\n",
    "    run_env[\"MPLBACKEND\"] = \"Agg\"\n",
    "    run_env[\"ORT_LOGGING_LEVEL\"] = \"3\"\n",
    "    src_path = str(REPO_ROOT / \"src\")\n",
    "    existing = run_env.get(\"PYTHONPATH\", \"\")\n",
    "    run_env[\"PYTHONPATH\"] = os.pathsep.join(filter(None, [src_path, str(REPO_ROOT), existing]))\n",
    "\n",
    "    run_shell_command(\n",
    "        pipeline_command,\n",
    "        env=run_env,\n",
    "        timeout=PIPELINE_TIMEOUT_SECONDS,\n",
    "        label=\"pipeline\",\n",
    "    )\n",
    "    print()\n",
    "\n",
    "    # --- Run backups ---\n",
    "    if RUN_BACKUPS:\n",
    "        print(\"[Stage 2/3] Running backups...\")\n",
    "        run_shell_command(\n",
    "            [sys.executable, \"scripts/backup_system.py\"],\n",
    "            timeout=3600,\n",
    "            label=\"backup\",\n",
    "        )\n",
    "        print()\n",
    "    else:\n",
    "        print(\"[Stage 2/3] Backups skipped (RUN_BACKUPS = False).\")\n",
    "        print()\n",
    "\n",
    "    # --- Output summary ---\n",
    "    print(\"[Stage 3/3] Output summary:\")\n",
    "    for label, location in output_locations:\n",
    "        loc_path = Path(str(location)).expanduser()\n",
    "        if loc_path.exists():\n",
    "            if loc_path.is_file():\n",
    "                size_mb = loc_path.stat().st_size / (1024 * 1024)\n",
    "                print(f\"  [OK] {label}: {size_mb:.2f} MB\")\n",
    "            else:\n",
    "                file_count = sum(1 for f in loc_path.rglob(\"*\") if f.is_file())\n",
    "                print(f\"  [OK] {label}: {file_count} file(s)\")\n",
    "        else:\n",
    "            print(f\"  [--] {label}: not found\")\n",
    "\n",
    "print()\n",
    "print(f\"Finished at {datetime.now():%Y-%m-%d %H:%M:%S}\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
