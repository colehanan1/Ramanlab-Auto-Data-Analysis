{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Pipeline Runner\n",
    "\n",
    "Run the complete pipeline automatically after YOLO model training.\n",
    "\n",
    "This notebook:\n",
    "1. Detects when a new YOLO model is trained\n",
    "2. Updates config with new model path\n",
    "3. Runs the full pipeline with backups\n",
    "4. Monitors progress and logs output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Project paths\n",
    "PROJECT_ROOT = Path('/home/ramanlab/Documents/cole/VSCode/Ramanlab-Auto-Data-Analysis')\n",
    "CONFIG_PATH = PROJECT_ROOT / 'config' / 'config.yaml'\n",
    "LOGS_DIR = PROJECT_ROOT / 'logs'\n",
    "\n",
    "# Create logs directory if needed\n",
    "LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"‚úì Project root: {PROJECT_ROOT}\")\n",
    "print(f\"‚úì Config path: {CONFIG_PATH}\")\n",
    "print(f\"‚úì Logs directory: {LOGS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Latest YOLO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_latest_yolo_model():\n",
    "    \"\"\"\n",
    "    Find the latest trained YOLO model.\n",
    "    Looks in the runs/ folder for the newest training.\n",
    "    \"\"\"\n",
    "    yolo_base = PROJECT_ROOT / 'model' / 'YOLOProjectProboscisLegs' / 'runs' / 'obb'\n",
    "    \n",
    "    if not yolo_base.exists():\n",
    "        print(f\"‚ùå YOLO models directory not found: {yolo_base}\")\n",
    "        return None\n",
    "    \n",
    "    # Find all training folders\n",
    "    train_dirs = sorted([d for d in yolo_base.iterdir() if d.is_dir() and d.name.startswith('train')])\n",
    "    \n",
    "    if not train_dirs:\n",
    "        print(f\"‚ùå No training folders found in {yolo_base}\")\n",
    "        return None\n",
    "    \n",
    "    latest = train_dirs[-1]\n",
    "    model_path = latest / 'weights' / 'best.pt'\n",
    "    \n",
    "    if model_path.exists():\n",
    "        print(f\"‚úì Found latest model: {latest.name}\")\n",
    "        print(f\"  Path: {model_path}\")\n",
    "        print(f\"  Size: {model_path.stat().st_size / (1024*1024):.1f} MB\")\n",
    "        print(f\"  Modified: {datetime.fromtimestamp(model_path.stat().st_mtime)}\")\n",
    "        return str(model_path)\n",
    "    else:\n",
    "        print(f\"‚ùå Model file not found: {model_path}\")\n",
    "        return None\n",
    "\n",
    "# Test finding model\n",
    "latest_model = find_latest_yolo_model()\n",
    "print(f\"\\nLatest model: {latest_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Config with New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_config_with_model(model_path):\n",
    "    \"\"\"\n",
    "    Update config.yaml with new model path.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read current config\n",
    "        with open(CONFIG_PATH, 'r') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "        \n",
    "        old_model = config.get('model_path', 'Unknown')\n",
    "        \n",
    "        # Update model path\n",
    "        config['model_path'] = model_path\n",
    "        \n",
    "        # Write updated config\n",
    "        with open(CONFIG_PATH, 'w') as f:\n",
    "            yaml.dump(config, f, default_flow_style=False)\n",
    "        \n",
    "        print(f\"‚úì Config updated successfully!\")\n",
    "        print(f\"  Old model: {old_model}\")\n",
    "        print(f\"  New model: {model_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error updating config: {e}\")\n",
    "        return False\n",
    "\n",
    "# Optional: update config if new model found\n",
    "if latest_model:\n",
    "    print(\"Ready to update config. Run next cell when ready.\")\n",
    "else:\n",
    "    print(\"‚ö† No model found - find your model path and enter it manually below.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option A: Auto-Update Config (Recommended)\n",
    "\n",
    "Update config with latest model automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-update with latest model\n",
    "if latest_model:\n",
    "    update_config_with_model(latest_model)\n",
    "else:\n",
    "    print(\"‚ùå No model found. Please manually enter model path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option B: Manual Model Path\n",
    "\n",
    "If auto-detect didn't work, paste your model path here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THIS with your model path\n",
    "manual_model_path = \"/home/ramanlab/Documents/cole/model/YOLOProjectProboscisLegs/runs/obb/train10/weights/best.pt\"\n",
    "\n",
    "# Uncomment to use manual path\n",
    "# update_config_with_model(manual_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline():\n",
    "    \"\"\"\n",
    "    Run the complete pipeline with make run.\n",
    "    Includes automatic CSV backups before and after.\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"STARTING PIPELINE\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Config: {CONFIG_PATH}\")\n",
    "    print(\"\\nThis will:\")\n",
    "    print(\"  1. Backup CSVs before processing\")\n",
    "    print(\"  2. Run full analysis pipeline\")\n",
    "    print(\"  3. Backup CSVs after processing\")\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    \n",
    "    # Create log file\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    log_file = LOGS_DIR / f\"pipeline_run_{timestamp}.log\"\n",
    "    \n",
    "    # Run make run\n",
    "    try:\n",
    "        # Change to project root for make command\n",
    "        os.chdir(PROJECT_ROOT)\n",
    "        \n",
    "        # Run with logging\n",
    "        with open(log_file, 'w') as log:\n",
    "            process = subprocess.Popen(\n",
    "                ['make', 'run'],\n",
    "                stdout=subprocess.PIPE,\n",
    "                stderr=subprocess.STDOUT,\n",
    "                text=True,\n",
    "                bufsize=1\n",
    "            )\n",
    "            \n",
    "            # Stream output in real-time\n",
    "            for line in process.stdout:\n",
    "                print(line, end='', flush=True)\n",
    "                log.write(line)\n",
    "            \n",
    "            returncode = process.wait()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        if returncode == 0:\n",
    "            print(\"‚úÖ PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "            print(f\"Log saved to: {log_file}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå PIPELINE FAILED with return code {returncode}\")\n",
    "            print(f\"Check log: {log_file}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error running pipeline: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        print(f\"\\nFinished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ EXECUTE: Run Full Pipeline\n",
    "\n",
    "**IMPORTANT:** This will take a while (15-60+ minutes depending on your data).\n",
    "\n",
    "The pipeline will:\n",
    "1. ‚úÖ Backup CSVs (before)\n",
    "2. üîÑ Run full analysis\n",
    "3. ‚úÖ Backup CSVs (after)\n",
    "\n",
    "Output will appear in real-time below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THE PIPELINE\n",
    "success = run_pipeline()\n",
    "\n",
    "if success:\n",
    "    print(\"\\n‚ú® Your analysis is complete and backed up!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Pipeline encountered an error. Check logs above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pipeline_results():\n",
    "    \"\"\"\n",
    "    Check what was generated by the pipeline.\n",
    "    \"\"\"\n",
    "    results_base = PROJECT_ROOT / 'Results'\n",
    "    \n",
    "    print(\"Pipeline Results:\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if results_base.exists():\n",
    "        # Count result files\n",
    "        results = list(results_base.rglob('*'))\n",
    "        files = [f for f in results if f.is_file()]\n",
    "        \n",
    "        print(f\"‚úì Results folder: {results_base}\")\n",
    "        print(f\"  Total items: {len(results)}\")\n",
    "        print(f\"  Files: {len(files)}\")\n",
    "        \n",
    "        # Show folder structure\n",
    "        if results:\n",
    "            print(\"\\nContents:\")\n",
    "            for item in sorted(results_base.iterdir()):\n",
    "                if item.is_dir():\n",
    "                    file_count = len(list(item.rglob('*')))\n",
    "                    print(f\"  üìÅ {item.name}/ ({file_count} items)\")\n",
    "    else:\n",
    "        print(f\"‚ùå Results folder not found: {results_base}\")\n",
    "    \n",
    "    # Check CSV outputs\n",
    "    csv_base = PROJECT_ROOT / 'Data' / 'Opto' / 'Combined'\n",
    "    print(f\"\\n\\nCSV Outputs:\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if csv_base.exists():\n",
    "        csvs = list(csv_base.glob('*.csv'))\n",
    "        print(f\"‚úì CSV folder: {csv_base}\")\n",
    "        print(f\"  CSV files: {len(csvs)}\")\n",
    "        \n",
    "        for csv in sorted(csvs):\n",
    "            size_mb = csv.stat().st_size / (1024*1024)\n",
    "            print(f\"    - {csv.name} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"‚ùå CSV folder not found: {csv_base}\")\n",
    "    \n",
    "    # Check if backups were created\n",
    "    print(f\"\\n\\nBackup Status:\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    backups_dir = PROJECT_ROOT / 'backups_compressed'\n",
    "    if backups_dir.exists():\n",
    "        zips = list(backups_dir.glob('*.zip'))\n",
    "        print(f\"‚úì Compressed archives: {len(zips)}\")\n",
    "        \n",
    "        for zip_file in sorted(zips, reverse=True)[:5]:  # Show last 5\n",
    "            size_mb = zip_file.stat().st_size / (1024*1024)\n",
    "            print(f\"    - {zip_file.name} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è  No compressed archives yet (run make backup-compressed to create)\")\n",
    "\n",
    "check_pipeline_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find latest pipeline log\n",
    "pipeline_logs = list(LOGS_DIR.glob('pipeline_run_*.log'))\n",
    "\n",
    "if pipeline_logs:\n",
    "    latest_log = sorted(pipeline_logs)[-1]\n",
    "    print(f\"Latest log: {latest_log.name}\")\n",
    "    print(f\"Size: {latest_log.stat().st_size / (1024*1024):.1f} MB\\n\")\n",
    "    \n",
    "    # Show last 50 lines\n",
    "    with open(latest_log, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    print(\"Last 50 lines of log:\")\n",
    "    print(\"=\"*70)\n",
    "    for line in lines[-50:]:\n",
    "        print(line, end='')\n",
    "else:\n",
    "    print(\"No pipeline logs found yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Commands\n",
    "\n",
    "Run these in separate cells:\n",
    "\n",
    "```python\n",
    "# Create compressed backup after pipeline\n",
    "os.chdir(PROJECT_ROOT)\n",
    "subprocess.run(['make', 'backup-compressed'])\n",
    "```\n",
    "\n",
    "```python\n",
    "# Check Box backups\n",
    "subprocess.run(['rclone', 'ls', 'Box-Folder:Ramanlab-Backups/'])\n",
    "```\n",
    "\n",
    "```python\n",
    "# Check logs\n",
    "subprocess.run(['tail', '-f', str(LOGS_DIR / 'backup.log')])\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
